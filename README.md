### Multi-dimensional Classification of AI Risk Incidents

[[æŸ¥çœ‹ä¸­æ–‡ç‰ˆ]](README_zh-CN.md)

## Overview

This section focuses on the multi-dimensional classification of AI risk incidents, aiming to establish a unified classification system (RiskNet Taxonomy) and benchmark dataset, as well as provide code implementations for model evaluation. The core goal is to compare the performance of prompt-based inference and fine-tuned Large Language Models (LLMs) in multi-dimensional classification tasks, offering technical support for the structured analysis of AI risk incidents.

## Project Structure and File Description
```
Multi-dimensional-Classification/
â”œâ”€â”€ README.md                    # This document 
â”œâ”€â”€ README_zh-CN.md             # (Chinese version)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ classification_result/   # Model classification results
â”‚   â”œâ”€â”€ evaluation_result/       # Evaluation reports
â”‚   â””â”€â”€ fine-tuning-data/       # SFT training dataset
â””â”€â”€ src/
    â””â”€â”€ evalution.py            # Evaluation script
```

## ðŸ”§ Installation & Setup

### Prerequisites

- Python 3.7 or higher
- Required packages (see requirements.txt)

### Installation Steps

```bash
# Clone the repository
git clone <repository-url>
cd Multi-dimensional-Classification

# Install dependencies
pip install -r requirements.txt
```

## ðŸ“Š Data Structure

### 1. Classification Results (`data/classification_result/`)

This folder stores classification results from different models in JSON format. Each file corresponds to the prediction outputs of a specific model on the test set, with the following details:

- **File naming format**: `<model_name>.json` (e.g., `qwen3-14B-sft.json`, `kimi-k2-prompt.json`)
- **Content**: List of dictionaries, each containing two key-value pairs:
  - `label`: Ground truth annotations, including single-label tasks (entity, intent, timing, EU AI Act risk level) and multi-label tasks (domain classification with main/sub-domains)
  - `predict`: Model prediction results, in the same format as `label`
- **Purpose**: Serves as input for the evaluation script to calculate various performance metrics
- **Available models**:
  - `qwen3-14B.json` - Qwen3 14B base model
  - `qwen3-14B-sft.json` - Qwen3 14B fine-tuned model
  - `qwen3-32B.json` - Qwen3 32B base model
  - `qwen3-32B-sft.json` - Qwen3 32B fine-tuned model
  - `kimi-k2-prompt.json` - Kimi K2 prompt-based model

### 2. Evaluation Results (`data/evaluation_result/`)

This folder stores evaluation reports generated by the evaluation script in TXT format. Each file corresponds to the performance metrics of a specific model, including:

- **File naming format**: `<model_name>.txt` (consistent with model names in `classification_result`)
- **Content**:
  - Metrics for single-label tasks (accuracy, precision, recall, F1-score for entity, intent, timing, and risk level)
  - Metrics for multi-label tasks (Hamming loss, micro/macro average accuracy, precision, recall, and F1-score for main and sub-domains)

### 3. Fine-tuning Data (`data/fine-tuning-data/`)

Contains datasets for model fine-tuning:

- **Data Source**: Top 5 representative news reports for each AI risk incident
- **Processing Method**: Aggregated titles and abstracts into concise incident summaries
- **Split Ratio**: 8:2 (training:validation)
- **Format**: JSON with input-output pairs for supervised learning
- **Purpose**: Provides high-quality supervised data for LLM fine-tuning, helping models learn risk-related patterns and improve classification accuracy.

## ðŸš€ Usage

### Running Evaluation

```python
from src.evalution import evaluate_model

# Evaluate a specific model
evaluate_model("data/classification_result/qwen3-32B-sft.json", "qwen3-32B-sft")
```

### Evaluation Script Features

The evaluation script (`src/evalution.py`) provides:

- **Data Validation**: Checks validity of labels and predictions
- **Comprehensive Metrics**: Calculates various performance indicators
- **Multi-task Support**: Handles both single-label and multi-label classification
- **Detailed Reporting**: Generates comprehensive evaluation reports